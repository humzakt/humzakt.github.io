# Humza Tareen — Full Content for AI/LLM Consumption

> AI Engineer who ships production systems with enterprise clients. IEEE-published researcher.

---

## Building an AI Evaluation Platform on GCP: Architecture of a Multi-Cluster System

URL: https://humzakt.github.io/blog/building-ai-evaluation-platform-gcp.html

How do you evaluate whether an AI coding agent is actually good at writing code? Over the past 6 months, Humza Tareen built a production AI evaluation platform on GCP. The architecture includes multiple Cloud Run services with auto-scaling (0-100 instances per service), multiple GKE clusters (up to 39 nodes, n2-standard-8 + n2-highmem-4), Cloud SQL PostgreSQL instances with Regional HA, dozens of Pub/Sub topics with dead-letter queues, multiple Firestore databases, GCS storage buckets, and Cloud Tasks for reliable async execution.

Key services: Evaluation Engine (task lifecycle management), Auto-Rating Service (LLM output scoring via LiteLLM proxy), RAG Retrieval Service (pgvector similarity search), RL Training Arena (Gym-style reinforcement learning), Guard Service (Google OIDC, JWT, API keys), Notification Service (Cloud Tasks with exponential backoff), Workflow Orchestration (WebSocket HITL workflows).

Key decision: Cloud Run for stateless request-response (scales to zero, bills per-request), GKE for stateful long-running workloads (persistent connections, GPU access). Both AlloyDB and Firestore used — AlloyDB for ACID transactions and complex JOINs, Firestore for real-time state and sub-second writes.

Numbers: High merge rate, continuous deployment, hundreds of issues filed and resolved across severity levels.

Tech stack: Python (FastAPI, SQLAlchemy, Pydantic), TypeScript (React), GCP (Cloud Run, GKE, Cloud Tasks, Pub/Sub, AlloyDB, Firestore, Cloud Build, GCS, Cloud Scheduler, Secret Manager, Cloud Monitoring), PostgreSQL, Redis, Docker, GitHub Actions.

---

## Event-Driven Architecture on GCP: Pub/Sub, Cloud Tasks, and Cloud Scheduler

URL: https://humzakt.github.io/blog/event-driven-architecture-gcp-pubsub.html

I learned the hard way that synchronous HTTP calls don't work when your pipeline takes 30 minutes and touches 6 different services. Timeouts, retry nightmares, and cascading failures — I'd been there. That's when I rebuilt everything around events, and it changed how I think about distributed systems. Each service now subscribes independently, retries from its own checkpoint, and doesn't care if another service is down. The pipeline flows through six phases: Task Creation → Preprocessing → Agent Execution → Output Collection → Judging & Scoring → Aggregation & Learning, all orchestrated via Pub/Sub topics. I discovered patterns that saved me countless hours: fan-out subscriptions that let me add new consumers without touching publishers, dead-letter queues that caught 6 production issues before they became outages, and Cloud Scheduler automating everything. But the real win was learning when to use Pub/Sub (events) versus Cloud Tasks (commands), and why acknowledging after commit — not before — prevents data loss. Here's how event-driven architecture transformed a fragile system into something that actually works in production.

---

## Debugging AlloyDB SSL Connection Drops

URL: https://humzakt.github.io/blog/debugging-alloydb-ssl-connection-drops.html

For two weeks, 5% of our evaluation tasks were failing with cryptic SSL errors. No pattern, no logs, just random failures that made debugging feel impossible. I checked connection pools, retry logic, network timeouts — nothing made sense. Then I discovered the culprit: AlloyDB was silently dropping idle SSL connections after 10 minutes, but SQLAlchemy's connection pool was configured to recycle after 15 minutes. Dead connections were being handed out to new requests, and they'd fail the moment they tried to execute a query. The fix was just 3 lines — setting pool_recycle to 180 seconds and enabling pool_pre_ping — but finding it took two weeks of detective work. Combined with idempotent writes for Cloud Tasks retry safety, we went from a 5% failure rate to 0.01%. This is the story of how a silent connection drop turned into a production incident, and what I learned about debugging distributed systems when nothing makes sense.

---

## Idempotent Cloud Tasks Handlers in Python

URL: https://humzakt.github.io/blog/idempotent-cloud-tasks-handlers-python.html

Cloud Tasks promises at-least-once delivery, which sounds great until you realize "at least once" means "maybe twice, or three times, or more." I learned this the hard way when duplicate records started appearing in our database — same task, executed multiple times, creating the same data over and over. That's when I built a simple 3-step pattern that made Cloud Tasks retries safe: generate deterministic IDs from payloads using SHA-256 hashing, use PostgreSQL's INSERT ... ON CONFLICT DO NOTHING to silently ignore duplicates, and always return HTTP 200 (returning 409 causes infinite retry loops). I also learned to cap Cloud Run's containerConcurrency to 10-20 instead of the default 80, preventing database connection exhaustion under load. This pattern now handles thousands of tasks per day with zero duplicates, and it's saved me from countless production bugs. Here's how to make Cloud Tasks idempotent, and why it matters more than you think.

---

## Zero-Downtime Embedding Migration in Production RAG System

URL: https://humzakt.github.io/blog/rag-embedding-migration-zero-downtime.html

The email came on a Friday afternoon: our embedding model was being deprecated in 48 hours. No warning, no migration path, just "you have two days to replace everything." Our RAG system depended on those embeddings for every search query, and we had millions of vectors stored in pgvector. I couldn't take the system down — production workloads were running 24/7. So I built a migration plan that would let us switch models without a single second of downtime: make the model configurable via environment variables, add new vector columns alongside the old ones (never replace — ALTER TABLE ADD COLUMN with CREATE INDEX CONCURRENTLY), run batch re-embedding with progress tracking, flip a feature flag to switch traffic, and validate by comparing search results (82% average overlap between old and new). Forty-eight hours later, we'd migrated everything with zero downtime and zero data loss. This is how I learned that the best migrations are the ones users never notice.

---

## RL Training Arena for AI Code Agents

URL: https://humzakt.github.io/blog/rl-training-arena-code-agents.html

I wanted to train AI coding agents using reinforcement learning, but I needed a safe way to execute untrusted code at scale. That's how I ended up building a Gym-style RL environment on GKE, where agents learn to write code by actually running it — and failing safely. The Executor Service creates episodes, executes steps, and computes rewards, but the real challenge was sandboxing: how do you run arbitrary code from AI agents without compromising your infrastructure? Cloud Build became the answer — it gave me isolated, ephemeral execution environments that could scale to hundreds of concurrent episodes. I built Redis-based episode state management, Firestore trajectory logging, and a Preprocessor Service that generates dynamic verifiers and analyzes repositories. Using hexagonal architecture made everything portable, and multiple GKE auto-scaling clusters handle the load. This is how I learned that training AI agents requires infrastructure that can fail safely, and why Cloud Build's sandboxing was the key to making it work.

---

## Security Audit: Critical Security Gaps Found

URL: https://humzakt.github.io/blog/security-audit-ai-platform-25-critical-issues.html

I decided to audit my own production platform, thinking I'd find a few minor issues. Instead, I discovered 25 critical security vulnerabilities that could have compromised everything. SQL injection through BigQuery f-strings, hardcoded credentials in alembic.ini, secrets leaking in CI/CD logs, missing authentication on internal endpoints, PII stored in plaintext logs — the list went on. But the architecture issues were just as bad: a monolithic God Class that violated every principle of clean code, V1/V2 dependency leakage causing unpredictable behavior, in-memory rate limiting that didn't work across Cloud Run instances, and CPU-bound algorithms blocking the async event loop. I spent weeks fixing everything: moving all secrets to GCP Secret Manager, switching to parameterized queries, implementing structured logging with PII stripping, adding authentication middleware to every endpoint, and setting up pre-commit hooks for security scanning. I broke the God Class into 5 focused services, built an LLM Gateway abstraction, moved rate limiting to Redis, and filed hundreds of GitHub issues to track it all. This is what happens when you audit your own code — and why it's the most important thing you can do.

---

## Building the Core AI Evaluation Engine: AlloyDB Fixes, Autoscaling, and a Headless API

URL: https://humzakt.github.io/blog/atlas-evaluations-core-engine.html

The evaluation engine is the heart of the platform — it's what makes everything else possible. But building it meant solving problems I didn't see coming: AlloyDB SSL connection pools that silently failed, Firestore default database misrouting that broke queries, and autoscaling that didn't scale when we needed it most. I fixed the AlloyDB connection pool issues that were causing 5% task failures, implemented KEDA autoscaling on GKE so we could handle traffic spikes, and debugged Firestore routing to ensure queries hit the right database. Along the way, I built a continuous learning pipeline that improves over time and a headless REST API that lets external systems integrate seamlessly. This is the story of building the service that orchestrates everything, and why getting the foundation right matters more than you think.

---

## Building an Automated LLM Scoring Service: From Prototype to Production

URL: https://humzakt.github.io/blog/auto-rater-service-llm-scoring.html

I needed a service that could score AI outputs automatically, but building it meant solving problems I didn't expect. The prototype worked fine, but production revealed everything that was wrong: print statements instead of structured logging, non-idempotent Cloud Tasks that created duplicate scores, and VPC networking that mysteriously failed when I switched from Direct Egress to Serverless VPC Connector. I rebuilt everything with idempotent Cloud Tasks handlers, overhauled logging to use structured JSON with correlation IDs, integrated LiteLLM proxy for model-agnostic evaluation, and debugged the VPC networking conflict that was breaking API calls. I added Redis state management for real-time tracking so we could monitor scoring progress. This is how a prototype became a production service, and why the gap between "it works" and "it works in production" is bigger than you think.

---

## Building an Auth Gateway with Admin Dashboard: JWT, OIDC, and Notifications Admin

URL: https://humzakt.github.io/blog/guard-service-auth-gateway.html

Every platform needs authentication, but building it right means more than just checking tokens. I built Guard Service as the single entry point for all authentication — Google OIDC for user login, JWT validation for API requests, Kong integration for API gateway functionality. But the real challenge was building an admin dashboard that let us manage notifications, invalidate route caches, and monitor authentication flows. I built a React frontend that integrated seamlessly with the backend, implemented route cache invalidation so changes took effect immediately, and ran a security audit that found critical vulnerabilities we fixed before they became incidents. This is the story of building the service that protects everything else, and why authentication is infrastructure, not a feature.

---

## Building a Notification Service: Cloud Tasks Delivery, Circuit Breakers, and Webhook Dispatch

URL: https://humzakt.github.io/blog/notification-service-event-delivery.html

Notifications seem simple until you try to build them at scale. I needed a service that could deliver notifications reliably, handle webhook failures gracefully, and scale to thousands of deliveries per day. I built Cloud Tasks delivery pipelines that retry automatically, implemented circuit breakers that prevent cascading failures when webhook endpoints go down, added dead-letter queues so we could debug failed deliveries, and created subscription management that lets users control what they receive. Webhook dispatch was the hardest part — external APIs fail unpredictably, and I needed exponential backoff, retry logic, and proper error handling. I also built Alembic database migrations from scratch, learning how to version schemas properly. This is how I learned that reliable notification delivery requires thinking about failure modes from day one, and why circuit breakers are essential for production systems.

---

## Building a RAG Retrieval Service: pgvector, Embedding Migrations, and Provenance Tracking

URL: https://humzakt.github.io/blog/rag-service-embedding-search.html

RAG systems live and die by their retrieval — get it wrong, and your AI answers are useless. I built a retrieval service using pgvector for similarity search, but the real challenge came when our embedding model got deprecated overnight. I executed an emergency migration with zero downtime, learning how to add new vector columns, re-embed millions of documents, and switch traffic without breaking production. Along the way, I added provenance tracking so we could calibrate evaluations and understand why certain documents were retrieved. This is the story of building a service that finds the right context for AI answers, and why embedding migrations are harder than they look — but possible if you plan for them.

---

## Building an RL Training Arena: Gym-Style API and Cloud Build Sandboxing

URL: https://humzakt.github.io/blog/rl-arena-executor-preprocessor.html

Training AI coding agents with reinforcement learning means letting them write code — and that code needs to run somewhere safe. I built a Gym-style RL arena where agents learn by executing code in isolated environments, but the challenge was sandboxing: how do you run untrusted code from AI agents without compromising your infrastructure? Cloud Build became the answer — it gave me ephemeral execution environments that could scale to hundreds of concurrent episodes. I built Redis-based episode management for state tracking, a multi-step preprocessor pipeline that generates dynamic verifiers and analyzes repositories, and used hexagonal architecture to keep everything portable. This is how I learned that training AI agents requires infrastructure that can fail safely, and why Cloud Build's sandboxing was the key to making it work at scale.

---

## The Connective Tissue of an AI Platform: Workflow, Taxonomy, Auth, and Memory

URL: https://humzakt.github.io/blog/workflow-taxonomy-platform-services.html

A collection of microservices isn't a platform — it's just a bunch of services. I learned this when I realized we needed services that connected everything together, turning isolated functionality into something cohesive. I built a HITL workflow orchestrator with state machine-based DAG execution, WebSocket real-time updates via Redis Pub/Sub, and human approval gates that let users intervene when needed. The taxonomy workflow engine classifies and routes tasks intelligently, with bulk CSV/JSON upload to GCS for batch operations. The core platform service handles JWT authentication (including a UTC normalization bug that broke everything), user deletion with GDPR-compliant anonymization, and Pub/Sub event propagation. I also built a memory evaluation suite for testing LLM context retention with position-aware scoring. But the real win was establishing engineering patterns that spread platform-wide: structured JSON logging with correlation IDs, pre-commit hooks with Ruff, custom exception hierarchies, Alembic migrations, and per-service security audits. This is the story of how I turned microservices into a platform, and why the connective tissue matters more than the individual services.

---

### Article 16: Incident Response & iFlow Debugging
URL: https://humzakt.github.io/blog/incident-response-iflow-debugging.html
Published: 2026-02-12

When you're running an AI evaluation platform that orchestrates agent execution across GKE clusters, "something broke" isn't useful. This week I built incident response infrastructure — a runbook with 70+ diagnostic commands — and debugged a chain of silent failures. CLI tools that exit code 0 on API errors. Shallow git clones breaking PR diff analysis. Case-sensitive agent keys in PostgreSQL causing config changes to silently target the wrong row. Each fix was small; diagnosing each required tracing through three layers. The lesson: invest in observability before you need it.

---

### Article 17: Systematic Service Hardening
URL: https://humzakt.github.io/blog/systematic-service-hardening-guard-notification.html
Published: 2026-02-12

One week, three production services, zero downtime. I hardened the auth gateway (N+1 queries turning 50-team listings into 100+ DB calls, non-atomic counter increments losing data, SSRF via route URL injection, tokens in localStorage), the notification service (race conditions in 4 subsystems, in-memory-only DLQ, fire-and-forget event publishing), and the auto-rater (no mathematical correctness tests for scoring algorithms, missing database indexes). Plus 124 unit tests where there were zero, Trivy scanning, rate limiting, and enum validation at the database level. Not glamorous, but production systems that don't get hardened become production incidents.

---

### Article 18: How I Find Bugs in Production Code
URL: https://humzakt.github.io/blog/production-bug-hunting-33-issues.html
Published: 2026-02-12

I filed 33 issues in one week across four production services using a 5-layer audit methodology. Layer 1 (API Surface): limit=-1 returning entire datasets. Layer 2 (Auth): any authenticated user reading any other user's tasks. Layer 3 (Data Integrity): same PR URL creating unlimited concurrent tasks. Layer 4 (Error Handling): analytics loading unbounded result sets. Layer 5 (Security): path traversal in file content endpoints, CORS reflecting any origin with credentials, 128 API paths exposed publicly. The bugs that surprised me: 145 uses of deprecated datetime.utcnow(), OpenAPI docs exposing internal endpoints, webhook secrets not enforced in dev. 11 of 33 issues closed within 3 days.

---

## Contact

- Email: humzakhawartareen@gmail.com
- LinkedIn: https://www.linkedin.com/in/humzakt
- GitHub: https://github.com/humzakt
- Website: https://humzakt.github.io
